{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load into bigquery\n",
    "\n",
    "Prototypes the `google.cloud.bigquery` python API to load table from parquets in GCS onto a bigquery dataset\n",
    "\n",
    "Note on `prefect-gcp` - there is an option to use prefect's client as a wrapper for GCP resources, but from my experience the documentation is a little lacking and I had to end up using the offical GCP docs anyway, so for sake of documentation completeness let's stick with official clients like I did for GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the parquets\n",
    "\n",
    "Use the `load_table_from_uri` func to load directly from cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbt_ny_taxi\n",
      "prod_ny_taxi\n",
      "service_call_test\n",
      "service_calls_dev\n",
      "test_service_calls_models\n",
      "trips_data_all\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")\n",
    "GCP_PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
    "\n",
    "# creates client with app default cred\n",
    "bq = bigquery.Client(location=\"us-west1\")\n",
    "for d in bq.list_datasets(project=GCP_PROJECT_ID):\n",
    "    print(d.dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 POST https://bigquery.googleapis.com/bigquery/v2/projects/de-zoom-83/datasets?prettyPrint=false: Already Exists: Dataset de-zoom-83:service_call_test\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"service_call_test\"\n",
    "dataset = bigquery.Dataset(dataset_ref=f\"{GCP_PROJECT_ID}.{dataset_id}\")\n",
    "try:\n",
    "    dataset = bq.create_dataset(dataset=dataset)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=de-zoom-83, location=us-west1, id=5b116bf7-b1ab-47ad-9025-9af74b10676e>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2021\n",
    "table_id = f\"facts_{year}_test\"\n",
    "src_uris = f\"gs://{DATA_LAKE}/raw/pq/SR{year}.parquet\"\n",
    "dest_table = bigquery.Table(f\"{GCP_PROJECT_ID}.{dataset_id}.{table_id}\")\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # schema=[\n",
    "    #     bigquery.SchemaField(\n",
    "    #         \"Status\", \"STRING\"\n",
    "    #     )\n",
    "    # ],\n",
    "    # source_format=\"PARQUET\",\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    ")\n",
    "# returns a LoadJob class\n",
    "job = bq.load_table_from_uri(\n",
    "    src_uris,\n",
    "    dest_table,\n",
    "    # job_id=\"test_notebook_client\", # if not spec'd, autogen\n",
    "    job_config=job_config,\n",
    "    location=\"us-west1\",\n",
    "    project=GCP_PROJECT_ID,\n",
    ")\n",
    "# call .result() to start\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323880 rows loaded\n"
     ]
    }
   ],
   "source": [
    "table = bq.get_table(dest_table)\n",
    "print(f'{table.num_rows} rows loaded')\n",
    "table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotchas:\n",
    "\n",
    "1. job_config is required?\n",
    "1. schema is required if table does not already exist, or if we're loading from Datastore\n",
    "1. If only certain fields are specified in schema, then only those columns are uploaded\n",
    "1. config needs to specify the source format as parquet; otherwise defaults to csv\n",
    "1. schema can be autodetected only if source is json or csv, *or parquet*\n",
    "1. [these docs on job config](https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationload) mention `sourceFormat`; `source_format` is a lucky guess.\n",
    "1. Never mind on points 2, 5. Needed to specify file format correctly [per sample code here](https://cloud.google.com/bigquery/docs/samples/bigquery-load-table-gcs-parquet#bigquery_load_table_gcs_parquet-python)\n",
    "\n",
    "Seems like a slight disconnect between the REST API types vs python sdk types, in how they're referenced in code, e.g. `sourceFormat` in REST, and `bigquery.SourceFormat` in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=de-zoom-83, location=us-west1, id=test_notebook_client>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq.cancel_job(job_id=\"test_notebook_client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq.delete_table(table=f\"{GCP_PROJECT_ID}.{dataset_id}.{table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq.delete_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JobConfig\n",
    "\n",
    "Next step: set partitioning and clustering in `job_config` during load: [code sample](https://cloud.google.com/bigquery/docs/samples/bigquery-load-table-clustered)\n",
    "\n",
    "After table is created:\n",
    "\n",
    "- [time partition](https://cloud.google.com/bigquery/docs/samples/bigquery-create-table-partitioned)\n",
    "- [clustering](https://cloud.google.com/bigquery/docs/samples/bigquery-create-table-clustered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 27cd73dc-0367-460a-a109-7b070cbc4c94 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while executing Future callback.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/klang/.cache/pypoetry/virtualenvs/to-service-qAcdV8BS-py3.10/lib/python3.10/site-packages/google/api_core/future/_helpers.py\", line 37, in safe_invoke_callback\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1795/2009562937.py\", line 16, in <lambda>\n",
      "    lambda x: print(f'Job duration: {time.time()-start_time}')\n",
      "TypeError: unsupported operand type(s) for -: 'float' and 'time.struct_time'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoadJob<project=de-zoom-83, location=us-west1, id=27cd73dc-0367-460a-a109-7b070cbc4c94>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2020\n",
    "table_id = f\"test_facts_{year}_partitioned\"\n",
    "src_uris = f\"gs://{DATA_LAKE}/raw/pq/SR{year}.parquet\"\n",
    "dest_table = bigquery.Table(f\"{GCP_PROJECT_ID}.{dataset_id}.{table_id}\")\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    "    time_partitioning=bigquery.TimePartitioning(\n",
    "        type_=bigquery.TimePartitioningType.DAY, field=\"creation_datetime\"\n",
    "    ),\n",
    "    clustering_fields=[\"Service_Request_Type\", \"ward_id\"],\n",
    ")\n",
    "job = bq.load_table_from_uri(src_uris, dest_table, job_config=job_config)\n",
    "start_time = time.time()\n",
    "print(f\"Job {job.job_id} started\")\n",
    "job.add_done_callback(lambda x: print(f\"Job duration: {time.time()-start_time}\"))\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-04-05 07:31:09.109000+00:00\n",
      "de-zoom-83.service_call_test.test_facts_2020_partitioned\n",
      "2023-04-05 07:31:11.140000+00:00\n",
      "load\n",
      "11871\n",
      "100\n",
      "13409\n",
      "us-west1\n",
      "DONE\n",
      "duration: 0:00:01.922000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_info = f\"\"\"\n",
    "{job.created}\n",
    "{job.destination}\n",
    "{job.ended}\n",
    "{job.job_type}\n",
    "{job.input_file_bytes}\n",
    "{job.output_rows}\n",
    "{job.output_bytes}\n",
    "{job.location}\n",
    "{job.state}\n",
    "duration: {job.ended - job.started}\n",
    "\"\"\"\n",
    "print(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 323880 rows and 10 columns to test_facts_2021_partitioned\n",
      "schema:\n",
      " [SchemaField('Status', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('First_3_Chars_of_Postal_Code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Intersection_Street_1', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Intersection_Street_2', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Service_Request_Type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Division', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('Section', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('ward_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('ward_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('creation_datetime', 'TIMESTAMP', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "table = bq.get_table(dest_table)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")\n",
    "print(\"schema:\\n\", table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id = \"service_call_test\"\n",
    "dataset_ref = bigquery.DatasetReference(GCP_PROJECT_ID, dataset_id)\n",
    "table_ref = dataset_ref.table(\"facts_2021_test\")\n",
    "table = bigquery.Table(table_ref)\n",
    "table.time_partitioning = bigquery.TimePartitioning(\n",
    "    type_=bigquery.TimePartitioningType.DAY,\n",
    "    field=\"creation_datetime\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating into pipeline\n",
    "\n",
    "- authentication?\n",
    "- timeout?\n",
    "- retries?\n",
    "- multipart?\n",
    "- check for existing?\n",
    "- ensure that the partitioning and clustering fields are present and valid?\n",
    "\n",
    "### Authenticate with service account key file\n",
    "\n",
    "Setup:\n",
    "\n",
    "- create service account with required permissions\n",
    "- download key file to executor\n",
    "- use `google.oauth2.service_account`:\n",
    "\n",
    "```py\n",
    "from google.oauth2 import service_account\n",
    "key_path = \"path/to/service_account.json\"\n",
    "cred = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "client = bigquery.Client(\n",
    "    credentials=creds,\n",
    "    project=cred.project_id,\n",
    ")\n",
    "```\n",
    "\n",
    "permissions needed:\n",
    "\n",
    "- bigquery.table.create, updateData, update\n",
    "- bigquery.jobs.create\n",
    "    - combine dataEditor and jobUser?\n",
    "- storage.buckets.get\n",
    "- storage.objects.get, list\n",
    "    - create from custom Storage Admin\n",
    "\n",
    "The following predefined IAM roles include the above permissions:\n",
    "\n",
    "- roles/bigquery.dataEditor, dataOwner (no job permission?), admin\n",
    "- bigqry.user, jobUser (doesn't include tables permission?)\n",
    "\n",
    "Integrate into terraform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client(location=\"canteen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canteen\n"
     ]
    }
   ],
   "source": [
    "print(client.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project_id', 'numeric_id', 'friendly_name', '__module__', '__doc__', '__init__', 'from_api_repr', '__dict__', '__weakref__', '__new__', '__repr__', '__hash__', '__str__', '__getattribute__', '__setattr__', '__delattr__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__reduce_ex__', '__reduce__', '__subclasshook__', '__init_subclass__', '__format__', '__sizeof__', '__dir__', '__class__']\n"
     ]
    }
   ],
   "source": [
    "for p in client.list_projects():\n",
    "    print(p.__dir__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-zoom-83:trips_data_all.fhv_taxi_trips\n",
      "de-zoom-83:trips_data_all.fhv_taxi_trips_ext\n",
      "de-zoom-83:trips_data_all.fhv_taxi_trips_partition\n",
      "de-zoom-83:trips_data_all.green-tip-model-eval\n",
      "de-zoom-83:trips_data_all.green-tip-model-feature-info\n",
      "de-zoom-83:trips_data_all.green-tip-model-predict\n",
      "de-zoom-83:trips_data_all.green-tip-model-predict-explain\n",
      "de-zoom-83:trips_data_all.green_taxi_trips\n",
      "de-zoom-83:trips_data_all.green_taxi_trips_ml\n",
      "de-zoom-83:trips_data_all.green_taxi_trips_partition\n",
      "de-zoom-83:trips_data_all.report_2020\n",
      "de-zoom-83:trips_data_all.yellow_taxi_trips\n",
      "de-zoom-83:trips_data_all.yellow_taxi_trips_ext\n"
     ]
    }
   ],
   "source": [
    "for t in client.list_tables(\"trips_data_all\"):\n",
    "    print(t.full_table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
