{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load into bigquery\n",
    "\n",
    "Prototypes the `google.cloud.bigquery` python API to load table from parquets in GCS onto a bigquery dataset\n",
    "\n",
    "Note on `prefect-gcp` - there is an option to use prefect's client as a wrapper for GCP resources, but from my experience the documentation is a little lacking and I had to end up using the offical GCP docs anyway, so for sake of documentation completeness let's stick with official clients like I did for GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the parquets\n",
    "\n",
    "Use the `load_table_from_uri` func to load directly from cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbt_ny_taxi\n",
      "prod_ny_taxi\n",
      "service_call_test\n",
      "trips_data_all\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")\n",
    "GCP_PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
    "\n",
    "# creates client with app default cred\n",
    "bq = bigquery.Client(location=\"us-west1\")\n",
    "for d in bq.list_datasets(project=GCP_PROJECT_ID):\n",
    "    print(d.dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"service_call_test\"\n",
    "dataset = bigquery.Dataset(dataset_ref=f\"{GCP_PROJECT_ID}.{dataset_id}\")\n",
    "try:\n",
    "    dataset = bq.create_dataset(dataset=dataset)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=de-zoom-83, location=us-west1, id=5b116bf7-b1ab-47ad-9025-9af74b10676e>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2021\n",
    "table_id = f\"facts_{year}_test\"\n",
    "src_uris = f\"gs://{DATA_LAKE}/raw/pq/SR{year}.parquet\"\n",
    "dest_table = bigquery.Table(f\"{GCP_PROJECT_ID}.{dataset_id}.{table_id}\")\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # schema=[\n",
    "    #     bigquery.SchemaField(\n",
    "    #         \"Status\", \"STRING\"\n",
    "    #     )\n",
    "    # ],\n",
    "    # source_format=\"PARQUET\",\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    ")\n",
    "# returns a LoadJob class\n",
    "job = bq.load_table_from_uri(\n",
    "    src_uris,\n",
    "    dest_table,\n",
    "    # job_id=\"test_notebook_client\", # if not spec'd, autogen\n",
    "    job_config=job_config,\n",
    "    location=\"us-west1\",\n",
    "    project=GCP_PROJECT_ID,\n",
    ")\n",
    "# call .result() to start\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323880 rows loaded\n"
     ]
    }
   ],
   "source": [
    "table = bq.get_table(dest_table)\n",
    "print(f'{table.num_rows} rows loaded')\n",
    "table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotchas:\n",
    "\n",
    "1. job_config is required?\n",
    "1. schema is required if table does not already exist, or if we're loading from Datastore\n",
    "1. If only certain fields are specified in schema, then only those columns are uploaded\n",
    "1. config needs to specify the source format as parquet; otherwise defaults to csv\n",
    "1. schema can be autodetected only if source is json or csv\n",
    "1. [these docs on job config](https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationload) mention `sourceFormat`; `source_format` is a lucky guess.\n",
    "1. Never mind on points 2, 5. Needed to specify file format correctly [per sample code here](https://cloud.google.com/bigquery/docs/samples/bigquery-load-table-gcs-parquet#bigquery_load_table_gcs_parquet-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=de-zoom-83, location=us-west1, id=test_notebook_client>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq.cancel_job(job_id=\"test_notebook_client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq.delete_table(table=f\"{GCP_PROJECT_ID}.{dataset_id}.{table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq.delete_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JobConfig\n",
    "\n",
    "Next step: set partitioning and clustering in `job_config`\n",
    "\n",
    "- [time partition](https://cloud.google.com/bigquery/docs/samples/bigquery-create-table-partitioned)\n",
    "- [clustering](https://cloud.google.com/bigquery/docs/samples/bigquery-create-table-clustered)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "to-service-qAcdV8BS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
