{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "Testing ground for service call data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "import folium\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 10:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- First 3 Chars of Postal Code: string (nullable = true)\n",
      " |-- Intersection Street 1: string (nullable = true)\n",
      " |-- Intersection Street 2: string (nullable = true)\n",
      " |-- Service Request Type: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- ward_name: string (nullable = true)\n",
      " |-- ward_id: byte (nullable = true)\n",
      " |-- creation_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq_path = Path(\"../tests/resources/SR2020.parquet\")\n",
    "df = spark.read.parquet(str(pq_path))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop current SparkContext before creating a new one\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Setup\n",
    "\n",
    "Read from cloud storage using `gcs-connector-hadoop3` jar\n",
    "\n",
    "Before starting a new context, I had to restart the kernel for spark to recognize the gcs-connector jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# !mkdir ../data/lib\n",
    "# !gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar  \\\n",
    "#     ../data/lib/gcs-connector-hadoop3-latest.jar\n",
    "# !curl -O https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.11/gcs-connector-hadoop3-2.2.11-shaded.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 13:47:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .setAppName(\"test_cloud\")\n",
    "    .set(\"spark.jars\", \"../data/lib/gcs-connector-hadoop3-latest.jar\")\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\n",
    "    \"fAbstractFileSystem.gimpl\", \"com.google.cloud.hadoop.fgcGoogleHadoopFS\"\n",
    ")\n",
    "hadoop_conf.set(\"fgimpl\", \"com.google.cloud.hadoop.fgcGoogleHadoopFileSystem\")\n",
    "spark = SparkSession.builder.config(conf=sc.getConf()).getOrCreate()\n",
    "load_dotenv()\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs_path: gs://service-calls-data-lake/raw/pq/sr2023.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- First 3 Chars of Postal Code: string (nullable = true)\n",
      " |-- Intersection Street 1: string (nullable = true)\n",
      " |-- Intersection Street 2: string (nullable = true)\n",
      " |-- Service Request Type: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- ward_name: string (nullable = true)\n",
      " |-- ward_id: byte (nullable = true)\n",
      " |-- creation_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gcs_path = f\"gs://{DATA_LAKE}/raw/pq/sr2023.parquet\"\n",
    "print(f\"gcs_path: {gcs_path}\")\n",
    "df_gcs = spark.read.parquet(gcs_path)\n",
    "df_gcs.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "How should we model our service call data?\n",
    "\n",
    "Most important fields:\n",
    "\n",
    "- Service Request Type\n",
    "- First 3 Chars of Postal Code\n",
    "- ward_name\n",
    "- datetime\n",
    "\n",
    "Ideas:\n",
    "\n",
    "- most common types per season?\n",
    "    - top 5?\n",
    "- most common types by ward/FSA?\n",
    "    - least common may not be as interesting, due to the high granularity in how types are categorized\n",
    "- highest frequency ward/FSA by service types, i.e. which ward has the most/least amount of noise complaints?\n",
    "    - for each service type (count > 100), top 3 most common ward?\n",
    "    - % of all requests for each ward?\n",
    "- combine with population per FSA and find requests per capita?\n",
    "- distributions of the all/top 80% of request types\n",
    "    - filter by ward/FSA/season?\n",
    "\n",
    "Division and section could be queried against to show which municipal entity receive the most requests, although that would highly correlate with the type of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 323880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "load_dotenv()\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")\n",
    "gcs_path = f\"gs://{DATA_LAKE}/raw/pq/SR2021.parquet\"\n",
    "df = spark.read.parquet(gcs_path)\n",
    "print(f\"row count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirement failed: Currently correlation calculation for columns with dataType string not supported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.corr(\"Service Request Type\", \"Division\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common request type by ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------------+-----+\n",
      "|               ward_name|                   Service Request Type|count|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141|\n",
      "|         Don Valley East|                     Property Standards|  349|\n",
      "|        Don Valley North|                                 Zoning|  413|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413|\n",
      "|         Don Valley West|                        General Pruning|  575|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|\n",
      "|         Etobicoke North|                     Property Standards|  517|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|\n",
      "|Humber River-Black Creek|                     Property Standards|  499|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531|\n",
      "|   Scarborough Southwest|                     Property Standards|  701|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|\n",
      "|   Scarborough-Guildwood|                     Property Standards|  484|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|\n",
      "|       Spadina-Fort York|                                 Zoning|  601|\n",
      "|          Toronto Centre|                    INJUR/DIST WILDLIFE|  651|\n",
      "|          Toronto Centre|                     Property Standards|  651|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|\n",
      "|              Willowdale|              Bin Investigation Request|  410|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459|\n",
      "|       York South-Weston|                     Property Standards|  716|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# request type count by ward\n",
    "df_ward_type = (\n",
    "    df.filter(df.ward_name.isNotNull())\n",
    "    .select(\"creation_datetime\", \"ward_name\", \"Service Request Type\")\n",
    "    .withColumnRenamed(\"Service Request Type\", \"service_request_type\")\n",
    "    .groupBy([\"ward_name\", \"service_request_type\"])\n",
    "    .count()\n",
    ")\n",
    "# most common request type's count by ward\n",
    "df_ward_max = (\n",
    "    df_ward_type.groupBy(\"ward_name\")\n",
    "    .max(\"count\")\n",
    "    .withColumnRenamed(\"max(count)\", \"max_count\")\n",
    ")\n",
    "\n",
    "# trying to join them get get most common type by ward\n",
    "df_ward = (\n",
    "    df_ward_max.join(df_ward_type, col(\"max_count\") == col(\"count\"), \"inner\")\n",
    "    .select(\"df_ward_max.ward_name\", \"Service Request Type\", \"count\")\n",
    "    .orderBy(\"ward_name\")\n",
    ")\n",
    "df_ward.show(n=30, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing some repeats of ward name due to counts of different request types being equal; is that a coincidence or is that really true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ward_max.orderBy(\"ward_name\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ward.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `Window` instead, as explained in [this SOF post](https://stackoverflow.com/questions/48829993/groupby-column-and-filter-rows-with-maximum-value-in-pyspark)\n",
    "\n",
    "- find max service type by ward\n",
    "- assign that value as a new column, according to ward\n",
    "- filter for rows where count == max_count\n",
    "- that row is the max_count service type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------------+-----+\n",
      "|               ward_name|                   Service Request Type|count|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141|\n",
      "|         Don Valley East|                     Property Standards|  349|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413|\n",
      "|         Don Valley West|                        General Pruning|  575|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|\n",
      "|         Etobicoke North|                     Property Standards|  517|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|\n",
      "|Humber River-Black Creek|                     Property Standards|  499|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531|\n",
      "|   Scarborough Southwest|                     Property Standards|  701|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|\n",
      "|       Spadina-Fort York|                                 Zoning|  601|\n",
      "|          Toronto Centre|                     Property Standards|  651|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459|\n",
      "|       York South-Weston|                     Property Standards|  716|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"ward_name\")\n",
    "df_ward_type.withColumn(\"max_count\", F.max(\"count\").over(w)).where(\n",
    "    F.col(\"count\") == F.col(\"max_count\")\n",
    ").drop(\"max_count\").show(n=25, truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "| ward_name|Service Request Type|count|\n",
      "+----------+--------------------+-----+\n",
      "|Willowdale|Long Grass and Weeds|  410|\n",
      "|Willowdale|Residential: Bin:...|  386|\n",
      "|Willowdale|     General Pruning|  375|\n",
      "|Willowdale|    CADAVER WILDLIFE|  313|\n",
      "|Willowdale| INJUR/DIST WILDLIFE|  285|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ward_type.filter(df_ward_type.ward_name == \"Willowdale\").orderBy(\n",
    "    \"count\", ascending=False\n",
    ").show(n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems my first approach picked up some erroneous row Simply matching the max_count and hoping they were unique was perhaps not the best way to go about it.\n",
    "\n",
    "## Window function\n",
    "\n",
    "Characterized by `OVER (PARTITION BY ...)`, there are 3 types:\n",
    "\n",
    "1. Aggregate\n",
    "1. Ranking\n",
    "1. Value\n",
    "\n",
    "### Aggregate\n",
    "\n",
    "Aggregates refer to:\n",
    "\n",
    "- AVG\n",
    "- MAX\n",
    "- MIN\n",
    "- SUM\n",
    "- COUNT\n",
    "\n",
    "This can be a replacement for `GROUP BY`, if we specify that group in the `OVER` clause. But unlike `GROUPBY.COUNT` which collapses the table into those groups, window assigns that value to each row of that dataset.\n",
    "\n",
    "In SQL for above query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_sql_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part one - creating table with count of each type for each ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                ward|                type|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|           Davenport|Residential: Bin:...| 1141|\n",
      "|   Eglinton-Lawrence|Residential: Bin:...|  984|\n",
      "|  Parkdale-High Park|Residential: Bin:...|  971|\n",
      "| Etobicoke-Lakeshore|     General Pruning|  913|\n",
      "|  Toronto-St. Paul's|Residential: Bin:...|  895|\n",
      "|    Etobicoke Centre|     General Pruning|  845|\n",
      "|    Toronto-Danforth|     General Pruning|  818|\n",
      "| University-Rosedale|Residential: Bin:...|  810|\n",
      "| Etobicoke-Lakeshore| INJUR/DIST WILDLIFE|  799|\n",
      "|   Beaches-East York|    CADAVER WILDLIFE|  788|\n",
      "|   Beaches-East York| INJUR/DIST WILDLIFE|  784|\n",
      "|   Beaches-East York|Residential: Bin:...|  774|\n",
      "|   Eglinton-Lawrence|     General Pruning|  751|\n",
      "|   Beaches-East York|     General Pruning|  737|\n",
      "|    Toronto-Danforth|    CADAVER WILDLIFE|  728|\n",
      "|   York South-Weston|  Property Standards|  716|\n",
      "|Scarborough South...|  Property Standards|  701|\n",
      "|    Toronto-Danforth|Residential: Bin:...|  699|\n",
      "|Scarborough-Rouge...|     General Pruning|  687|\n",
      "|   Beaches-East York|  Property Standards|  683|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "df_type_count = spark.sql(query)\n",
    "df_type_count.createOrReplaceTempView(\"df_type_count\")\n",
    "df_type_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part two - using common table expression (CTE) to add a column with max(count) of each ward, and query against it to find the max count service type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                ward|                type|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|   Beaches-East York|    CADAVER WILDLIFE|  788|\n",
      "|           Davenport|Residential: Bin:...| 1141|\n",
      "|     Don Valley East|  Property Standards|  349|\n",
      "|    Don Valley North|Residential: Bin:...|  413|\n",
      "|     Don Valley West|     General Pruning|  575|\n",
      "|   Eglinton-Lawrence|Residential: Bin:...|  984|\n",
      "|    Etobicoke Centre|     General Pruning|  845|\n",
      "|     Etobicoke North|  Property Standards|  517|\n",
      "| Etobicoke-Lakeshore|     General Pruning|  913|\n",
      "|Humber River-Blac...|  Property Standards|  499|\n",
      "|  Parkdale-High Park|Residential: Bin:...|  971|\n",
      "|  Scarborough Centre|Residential: Bin:...|  668|\n",
      "|   Scarborough North|Residential: Bin:...|  531|\n",
      "|Scarborough South...|  Property Standards|  701|\n",
      "|Scarborough-Aginc...|     General Pruning|  428|\n",
      "|Scarborough-Guild...| INJUR/DIST WILDLIFE|  484|\n",
      "|Scarborough-Rouge...|     General Pruning|  687|\n",
      "|   Spadina-Fort York|              Zoning|  601|\n",
      "|      Toronto Centre|  Property Standards|  651|\n",
      "|    Toronto-Danforth|     General Pruning|  818|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH type_max AS (\n",
    "    SELECT \n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        MAX(count) OVER (PARTITION BY ward) maxcount\n",
    "    FROM df_type_count\n",
    ")\n",
    "SELECT\n",
    "    ward, type, count\n",
    "FROM\n",
    "    type_max\n",
    "WHERE count == maxcount\n",
    "\"\"\"\n",
    "df_type_max = spark.sql(query)\n",
    "df_type_max.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together. CTE used to construct a table with `maxcount` column to query against so that we can use it in the `WHERE` statement; `WHERE` cannot accept windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:47:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:47:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------------------------+---------------------------------------+-----+---+\n",
      "|                    ward|                                   type|count|row|\n",
      "+------------------------+---------------------------------------+-----+---+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|  1|\n",
      "|         Don Valley West|                        General Pruning|  575|  2|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|  3|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|  4|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|  5|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|  6|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|  7|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|  8|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|  9|\n",
      "|         Don Valley East|                     Property Standards|  349| 10|\n",
      "|         Etobicoke North|                     Property Standards|  517| 11|\n",
      "|Humber River-Black Creek|                     Property Standards|  499| 12|\n",
      "|   Scarborough Southwest|                     Property Standards|  701| 13|\n",
      "|          Toronto Centre|                     Property Standards|  651| 14|\n",
      "|       York South-Weston|                     Property Standards|  716| 15|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141| 16|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413| 17|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984| 18|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971| 19|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668| 20|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531| 21|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895| 22|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810| 23|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459| 24|\n",
      "|       Spadina-Fort York|                                 Zoning|  601| 25|\n",
      "+------------------------+---------------------------------------+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_query = \"\"\"\n",
    "with type_count as (\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "),\n",
    "type_max as (\n",
    "    SELECT \n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        MAX(count) OVER (PARTITION BY ward) maxcount\n",
    "    FROM type_count\n",
    ")\n",
    "SELECT\n",
    "    ward, type, count, row_number() over(ORDER BY type) row\n",
    "FROM\n",
    "    type_max\n",
    "WHERE count == maxcount\n",
    "ORDER BY type\n",
    "\"\"\"\n",
    "df_sql = spark.sql(window_query).show(n=25, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common ward by request type\n",
    "\n",
    "Inverse of the previous question - which neighborhood makes the most calls for each request type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                type|                ward|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|All / Hazardous W...|  Parkdale-High Park|  656|\n",
      "|Bin Investigation...|           Davenport|  511|\n",
      "|Boulevard - Ploug...|Scarborough-Guild...|  102|\n",
      "|Boulevards - Dama...|    Etobicoke Centre|  194|\n",
      "|By-Law Contravent...| Etobicoke-Lakeshore|  126|\n",
      "|    CADAVER WILDLIFE|   Beaches-East York|  788|\n",
      "|Catch Basin - Blo...|   Eglinton-Lawrence|  134|\n",
      "|Comment / Suggestion|   Spadina-Fort York|  118|\n",
      "|Complaint/Investi...|           Davenport|  179|\n",
      "|Dispute SR Status...|   Eglinton-Lawrence|  211|\n",
      "|      Dogs off Leash|   Spadina-Fort York|  131|\n",
      "|Expressway requir...|   Spadina-Fort York|  122|\n",
      "|     General Pruning| Etobicoke-Lakeshore|  913|\n",
      "|General Tree Main...| Etobicoke-Lakeshore|  115|\n",
      "|            Graffiti| University-Rosedale|  290|\n",
      "|Gypsy Moth Contro...|    Don Valley North|  168|\n",
      "| INJUR/DIST WILDLIFE| Etobicoke-Lakeshore|  799|\n",
      "|Ice and Snow Comp...| University-Rosedale|  143|\n",
      "|Litter / Bin / Ov...|   Spadina-Fort York|  331|\n",
      "|Litter / Illegal ...| University-Rosedale|  323|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ward_query = \"\"\"\n",
    "with type_count as (\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "),\n",
    "ward_max as (\n",
    "    SELECT \n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        MAX(count) OVER (PARTITION BY type) maxcount\n",
    "    FROM type_count\n",
    ")\n",
    "SELECT\n",
    "    type, ward, count\n",
    "FROM ward_max\n",
    "WHERE maxcount = count and count > 100\n",
    "\"\"\"\n",
    "df_ward_by_type = spark.sql(ward_query).show()\n",
    "# df_ward_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 requests by season\n",
    "\n",
    "Let's define seasons by datetime:\n",
    "\n",
    "- spring: 3/21-6/21\n",
    "- summer: 6/21-9/21\n",
    "- fall: 9/21 - 11/30 (winter is long, okay)\n",
    "- winter: 12/1 - 3/21\n",
    "\n",
    "Execution:\n",
    "\n",
    "- add season column via UDF\n",
    "- for each season, count rows by service type\n",
    "- since we want the top $n$ service type by season, we can't just use `max`\n",
    "    - for each season, sort the count\n",
    "    - get row numbers 1-5 for each season - use window\n",
    "    \n",
    "Alternative - SQL execution?\n",
    "\n",
    "- CASE/WHEN/THEN to extract season\n",
    "- `row_number() OVER(PARTITION BY season)`\n",
    "\n",
    "#### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  creation_datetime|\n",
      "+-------------------+\n",
      "|2021-01-01 09:45:45|\n",
      "|2021-01-02 09:34:10|\n",
      "|2021-01-02 11:07:43|\n",
      "|2021-01-02 12:47:47|\n",
      "|2021-01-02 13:53:55|\n",
      "|2021-01-02 16:20:38|\n",
      "|2021-01-03 09:12:28|\n",
      "|2021-01-03 12:49:48|\n",
      "|2021-01-03 18:55:33|\n",
      "|2021-01-04 11:36:54|\n",
      "|2021-01-04 12:01:47|\n",
      "|2021-01-04 17:18:47|\n",
      "|2021-01-04 20:58:35|\n",
      "|2021-01-05 08:13:23|\n",
      "|2021-01-05 09:29:38|\n",
      "|2021-01-05 10:21:36|\n",
      "|2021-01-05 11:40:57|\n",
      "|2021-01-05 12:22:56|\n",
      "|2021-01-05 13:30:11|\n",
      "|2021-01-06 12:25:47|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dt_samp, _ = df.randomSplit([0.1,0.9])\n",
    "dt_samp = df.sample(0.005).select(\"creation_datetime\")\n",
    "dt_samp.createOrReplaceTempView(\"dt_samp\")\n",
    "dt_samp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1649"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_samp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1 = dt_samp.head(1)\n",
    "type(dt1[0].creation_datetime)\n",
    "dtval = dt1[0].creation_datetime\n",
    "dtval.year\n",
    "spring = datetime(dtval.year, month=3, day=21)\n",
    "dtval < spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_season(creation_time: datetime) -> str:\n",
    "    \"\"\"Extracts season from the datetime field\n",
    "\n",
    "    Use as spark UDF\n",
    "    \"\"\"\n",
    "    m = creation_time.month\n",
    "    d = creation_time.day\n",
    "    if m < 3 or (m == 3 and d < 21) or m > 11:\n",
    "        return \"winter\"\n",
    "    elif m < 6 or (m == 6 and d < 21):\n",
    "        return \"spring\"\n",
    "    elif m < 9 or (m == 9 and d < 21):\n",
    "        return \"summer\"\n",
    "    else:\n",
    "        return \"fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiate spark UDF\n",
    "extract_season_udf = F.udf(extract_season, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|  creation_datetime|season|\n",
      "+-------------------+------+\n",
      "|2021-01-16 13:04:01|winter|\n",
      "|2021-01-19 18:30:09|winter|\n",
      "|2021-02-23 13:31:35|winter|\n",
      "|2021-03-25 11:49:37|spring|\n",
      "|2021-04-04 10:39:06|spring|\n",
      "|2021-04-21 12:51:34|spring|\n",
      "|2021-04-23 15:40:24|spring|\n",
      "|2021-05-11 11:46:21|spring|\n",
      "|2021-06-03 14:56:26|spring|\n",
      "|2021-06-04 17:00:54|spring|\n",
      "|2021-06-10 09:11:18|spring|\n",
      "|2021-06-11 15:36:45|spring|\n",
      "|2021-06-14 22:29:32|spring|\n",
      "|2021-06-18 18:13:40|spring|\n",
      "|2021-06-23 08:10:08|summer|\n",
      "|2021-06-23 11:03:17|summer|\n",
      "|2021-06-30 14:10:12|summer|\n",
      "|2021-07-01 09:05:45|summer|\n",
      "|2021-07-16 16:11:03|summer|\n",
      "|2021-07-17 19:29:50|summer|\n",
      "|2021-07-21 13:59:00|summer|\n",
      "|2021-08-05 15:40:58|summer|\n",
      "|2021-08-30 09:03:25|summer|\n",
      "|2021-09-02 09:23:01|summer|\n",
      "|2021-10-20 19:22:25|  fall|\n",
      "|2021-10-28 13:21:34|  fall|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# call UDF on the creation_datetime column\n",
    "dt_samp.withColumn(\"season\", extract_season_udf(dt_samp.creation_datetime)).sample(\n",
    "    0.02\n",
    ").show(n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------+-----+-------------+\n",
      "|season|                   service_request_type|count|seasonal_rank|\n",
      "+------+---------------------------------------+-----+-------------+\n",
      "|  fall|Residential: Bin: Repair or Replace Lid|  186|            1|\n",
      "|  fall|                     Property Standards|  179|            2|\n",
      "|  fall|                       CADAVER WILDLIFE|  167|            3|\n",
      "|spring|                        General Pruning|  448|            1|\n",
      "|spring|Residential: Bin: Repair or Replace Lid|  446|            2|\n",
      "|spring|                    INJUR/DIST WILDLIFE|  419|            3|\n",
      "|summer|                        General Pruning|  614|            1|\n",
      "|summer|Residential: Bin: Repair or Replace Lid|  481|            2|\n",
      "|summer|                       CADAVER WILDLIFE|  451|            3|\n",
      "|winter|                    INJUR/DIST WILDLIFE|  319|            1|\n",
      "|winter|                                 Zoning|  309|            2|\n",
      "|winter|Residential: Bin: Repair or Replace Lid|  264|            3|\n",
      "+------+---------------------------------------+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify in window that `count` must be ordered as descending\n",
    "# by referencing the COLUMN and using the `.desc()` method\n",
    "w = Window.partitionBy(\"season\").orderBy(F.col(\"count\").desc())\n",
    "df_season_type = (\n",
    "    df.sample(0.1)\n",
    "    .withColumnRenamed(\"Service Request Type\", \"service_request_type\")\n",
    "    .withColumn(\"season\", extract_season_udf(df.creation_datetime))\n",
    "    .groupby(\"season\", \"service_request_type\")\n",
    "    .count()\n",
    "    .withColumn(\"seasonal_rank\", F.row_number().over(w))\n",
    "    .filter(F.col(\"seasonal_rank\") < 4)\n",
    "    .show(truncate=40)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank vs Dense Rank vs Row\n",
    "\n",
    "Differs in how they handle ties:\n",
    "\n",
    "|col1|col2|rank|dense_rank|row_number|\n",
    "|----|----|----|----------|----------|\n",
    "|   a|  10|   1|         1|         1|\n",
    "|   a|  10|   1|         1|         2|\n",
    "|   a|  20|   3|         2|         3|\n",
    "\n",
    "`row_number` will always give unique IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL CASE/WHEN\n",
    "\n",
    "Extracting season with `CASE/WHEN/THEN/ELSE` in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_query = \"\"\"\n",
    "    SELECT\n",
    "        creation_datetime,\n",
    "        CASE\n",
    "            WHEN MONTH(creation_datetime) < 3 OR MONTH(creation_datetime) > 11 OR\n",
    "            MONTH(creation_datetime) = 3 AND DAY(creation_datetime) < 21 THEN 'winter'\n",
    "            WHEN MONTH(creation_datetime) < 6 OR MONTH(creation_datetime) = 6 AND DAY(creation_datetime) < 21 THEN 'spring'\n",
    "            WHEN MONTH(creation_datetime) < 9 OR MONTH(creation_datetime) = 9 AND DAY(creation_datetime) < 21 THEN 'summer'\n",
    "            ELSE 'fall'\n",
    "        END as season,\n",
    "        count(1) count\n",
    "    FROM\n",
    "        dt_samp\n",
    "    GROUP BY season\n",
    "    TABLESAMPLE(0.1 PERCENT)\n",
    "\"\"\"\n",
    "sql_season = spark.sql(season_query)\n",
    "sql_season.sample(0.02).show(n=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. First CTE `season_type_count` creates the count column by aggregating season and types\n",
    "1. Second CTE `season_rank_table` creates the `seasonal_rank` column using window function `row_number()`\n",
    "1. Final query filters the `seasonal_rank` column. Window functions can only be used in `SELECT` clauses, and so if we want to filter the window function return values, we create the CTE to query against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+-------------+\n",
      "|season|service_request_type|count|seasonal_rank|\n",
      "+------+--------------------+-----+-------------+\n",
      "|  fall|  Property Standards|  313|            1|\n",
      "|  fall|Residential: Bin:...|  302|            2|\n",
      "|  fall|    CADAVER WILDLIFE|  282|            3|\n",
      "|spring|Residential: Bin:...|  434|            1|\n",
      "|spring| INJUR/DIST WILDLIFE|  383|            2|\n",
      "|spring|              Zoning|  347|            3|\n",
      "|summer|     General Pruning|  665|            1|\n",
      "|summer|Residential: Bin:...|  518|            2|\n",
      "|summer|  Property Standards|  453|            3|\n",
      "|winter|Residential: Bin:...|  213|            1|\n",
      "|winter|              Zoning|  210|            2|\n",
      "|winter| INJUR/DIST WILDLIFE|  204|            3|\n",
      "+------+--------------------+-----+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "season_sort = \"\"\"\n",
    "    WITH season_type_count AS (\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN MONTH(creation_datetime) < 3 OR MONTH(creation_datetime) > 11 THEN 'winter'\n",
    "                WHEN MONTH(creation_datetime) < 6 THEN 'spring'\n",
    "                WHEN MONTH(creation_datetime) < 9 THEN 'summer'\n",
    "                ELSE 'fall'\n",
    "            END season,\n",
    "            `Service Request Type` service_request_type,\n",
    "            COUNT(1) count\n",
    "        FROM\n",
    "            df_sql_view\n",
    "        TABLESAMPLE (10 PERCENT)\n",
    "        GROUP BY season, `Service Request Type`\n",
    "    ),\n",
    "    season_rank_table AS(\n",
    "        SELECT\n",
    "            season, \n",
    "            service_request_type,\n",
    "            count,\n",
    "            ROW_NUMBER() OVER(PARTITION BY season ORDER BY count DESC) seasonal_rank\n",
    "        FROM season_type_count\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM season_rank_table\n",
    "    WHERE seasonal_rank < 4\n",
    "\"\"\"\n",
    "spark.sql(season_sort).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add proportion when displaying the rank\n",
    "\n",
    "- Calculate new column `total` using the window function over `count`\n",
    "- Another column `percentage` from `count / total`\n",
    "- carry `percentage` to the final table\n",
    "- to format `percentage`, need to query it from another table; can't do `CAST(count / subtotal ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-----+--------+-------------+----------+\n",
      "|season|service_request_type|count|subtotal|seasonal_rank|percentage|\n",
      "+------+--------------------+-----+--------+-------------+----------+\n",
      "|  fall|  Property Standards|  151|    3413|            1|     0.044|\n",
      "|  fall|     General Pruning|  133|    3413|            2|     0.039|\n",
      "|  fall|Residential: Bin:...|  127|    3413|            3|     0.037|\n",
      "|spring| INJUR/DIST WILDLIFE|  226|    4864|            1|     0.046|\n",
      "|spring|Residential: Bin:...|  220|    4864|            2|     0.045|\n",
      "|spring|              Zoning|  184|    4864|            3|     0.038|\n",
      "|summer|     General Pruning|  340|    5711|            1|     0.060|\n",
      "|summer|Residential: Bin:...|  251|    5711|            2|     0.044|\n",
      "|summer|    CADAVER WILDLIFE|  219|    5711|            3|     0.038|\n",
      "|winter|Residential: Bin:...|  105|    2417|            1|     0.043|\n",
      "|winter|              Zoning|  103|    2417|            2|     0.043|\n",
      "|winter| INJUR/DIST WILDLIFE|   88|    2417|            3|     0.036|\n",
      "+------+--------------------+-----+--------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "season_sort = \"\"\"\n",
    "    WITH season_type_count AS (\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN MONTH(creation_datetime) < 3 OR MONTH(creation_datetime) > 11 THEN 'winter'\n",
    "                WHEN MONTH(creation_datetime) < 6 THEN 'spring'\n",
    "                WHEN MONTH(creation_datetime) < 9 THEN 'summer'\n",
    "                ELSE 'fall'\n",
    "            END AS season,\n",
    "            `Service Request Type` AS service_request_type,\n",
    "            COUNT(1) AS count\n",
    "        FROM\n",
    "            df_sql_view\n",
    "        TABLESAMPLE (5 PERCENT)\n",
    "        GROUP BY season, `Service Request Type`\n",
    "    ),\n",
    "    season_rank_table AS(\n",
    "        SELECT\n",
    "            season, \n",
    "            service_request_type,\n",
    "            count,\n",
    "            SUM(count) OVER(PARTITION BY season) AS subtotal,\n",
    "            ROW_NUMBER() OVER(PARTITION BY season ORDER BY count DESC) AS seasonal_rank,\n",
    "            count / SUM(count) OVER(PARTITION BY season) AS percentage\n",
    "        FROM season_type_count\n",
    "    )\n",
    "    SELECT \n",
    "        season, \n",
    "        service_request_type,\n",
    "        count,\n",
    "        seasonal_rank,\n",
    "        CAST(percentage AS DECIMAL(4, 3))\n",
    "    FROM season_rank_table\n",
    "    WHERE seasonal_rank < 4\n",
    "    ORDER BY season, seasonal_rank ASC;\n",
    "\"\"\"\n",
    "spark.sql(season_sort).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common types by ward - top 5\n",
    "\n",
    "Instead of only returning the most common (max), return the top 5 most common types per ward, using the window function, and include the proportions\n",
    "\n",
    "- Filter by substring - use `WHERE name LIKE 'str_pattern'`\n",
    "    - `%` zero or more\n",
    "    - `_` one\n",
    "    - `[abc]` any of a, b, or c\n",
    "    - `[^abc]` matches all char that are *not* a, b, or c\n",
    "    - `[a-z]` matches all single char in that range, i.e. all lower case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------------------------+-----+---------+----------+\n",
      "|                  ward|                                   type|count|ward_rank|percentage|\n",
      "+----------------------+---------------------------------------+-----+---------+----------+\n",
      "|    Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668|        1|     0.055|\n",
      "|    Scarborough Centre|                     Property Standards|  484|        2|     0.040|\n",
      "|    Scarborough Centre|                       CADAVER WILDLIFE|  443|        3|     0.037|\n",
      "|     Scarborough North|Residential: Bin: Repair or Replace Lid|  531|        1|     0.067|\n",
      "|     Scarborough North|                        General Pruning|  377|        2|     0.048|\n",
      "|     Scarborough North|Res / Organic Green Bin / Not Picked Up|  300|        3|     0.038|\n",
      "| Scarborough Southwest|                     Property Standards|  701|        1|     0.047|\n",
      "| Scarborough Southwest|                       CADAVER WILDLIFE|  588|        2|     0.039|\n",
      "| Scarborough Southwest|          Res / Garbage / Not Picked Up|  557|        3|     0.037|\n",
      "| Scarborough-Agincourt|                        General Pruning|  428|        1|     0.055|\n",
      "| Scarborough-Agincourt|Residential: Bin: Repair or Replace Lid|  421|        2|     0.054|\n",
      "| Scarborough-Agincourt|                                 Zoning|  297|        3|     0.038|\n",
      "| Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|        1|     0.044|\n",
      "| Scarborough-Guildwood|Residential: Bin: Repair or Replace Lid|  441|        2|     0.040|\n",
      "| Scarborough-Guildwood|                     Property Standards|  422|        3|     0.039|\n",
      "|Scarborough-Rouge Park|                        General Pruning|  687|        1|     0.054|\n",
      "|Scarborough-Rouge Park|Residential: Bin: Repair or Replace Lid|  623|        2|     0.049|\n",
      "|Scarborough-Rouge Park|          Res / Garbage / Not Picked Up|  587|        3|     0.046|\n",
      "+----------------------+---------------------------------------+-----+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ward_top_query = \"\"\"\n",
    "with ward_count as (\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "),\n",
    "ward_rank_table as (\n",
    "    SELECT\n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        ROW_NUMBER() OVER(PARTITION BY ward ORDER BY count DESC) ward_rank,\n",
    "        count / SUM(count) OVER(PARTITION BY ward) percentage\n",
    "    FROM\n",
    "        ward_count\n",
    ")\n",
    "SELECT\n",
    "    ward,\n",
    "    type,\n",
    "    count,\n",
    "    ward_rank,\n",
    "    CAST(percentage AS DECIMAL(4,3))\n",
    "FROM ward_rank_table\n",
    "WHERE ward_rank < 4 AND ward LIKE 'Scar%'\n",
    "ORDER BY ward, ward_rank\n",
    "\"\"\"\n",
    "spark.sql(ward_top_query).show(truncate=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top wards by request types\n",
    "\n",
    "Same as above, but group by types instead of wards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------------------------+-----+---------+----------+\n",
      "|                 ward|                                  type|count|type_rank|percentage|\n",
      "+---------------------+--------------------------------------+-----+---------+----------+\n",
      "|            Davenport|             Bin Investigation Request|  511|        1|     0.130|\n",
      "|  University-Rosedale|             Bin Investigation Request|  411|        2|     0.105|\n",
      "|   Parkdale-High Park|             Bin Investigation Request|  410|        3|     0.104|\n",
      "|Scarborough-Guildwood|             Boulevard - Plough Damage|  102|        1|     1.000|\n",
      "|     Etobicoke Centre|          Boulevards - Damaged Asphalt|  194|        1|     0.240|\n",
      "|  Etobicoke-Lakeshore|          Boulevards - Damaged Asphalt|  148|        2|     0.183|\n",
      "|    Eglinton-Lawrence|          Boulevards - Damaged Asphalt|  147|        3|     0.182|\n",
      "|  Etobicoke-Lakeshore|           By-Law Contravention Invest|  126|        1|     0.275|\n",
      "|    Beaches-East York|           By-Law Contravention Invest|  118|        2|     0.257|\n",
      "|     Etobicoke Centre|           By-Law Contravention Invest|  113|        3|     0.246|\n",
      "|    Eglinton-Lawrence|      Catch Basin - Blocked / Flooding|  134|        1|     0.287|\n",
      "|  Etobicoke-Lakeshore|      Catch Basin - Blocked / Flooding|  117|        2|     0.251|\n",
      "|   Toronto-St. Paul's|      Catch Basin - Blocked / Flooding|  112|        3|     0.240|\n",
      "|    Spadina-Fort York|                  Comment / Suggestion|  118|        1|     1.000|\n",
      "|            Davenport|Complaint/Investigation - Encroachment|  179|        1|     0.113|\n",
      "|  University-Rosedale|Complaint/Investigation - Encroachment|  165|        2|     0.104|\n",
      "|     Toronto-Danforth|Complaint/Investigation - Encroachment|  159|        3|     0.100|\n",
      "|    Eglinton-Lawrence|Dispute SR Status/Collections Curb Day|  211|        1|     0.181|\n",
      "|    York South-Weston|Dispute SR Status/Collections Curb Day|  143|        2|     0.122|\n",
      "|  Etobicoke-Lakeshore|Dispute SR Status/Collections Curb Day|  142|        3|     0.122|\n",
      "+---------------------+--------------------------------------+-----+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "types_top_query = \"\"\"\n",
    "with types_count as (\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "),\n",
    "types_rank_table as (\n",
    "    SELECT\n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        ROW_NUMBER() OVER(PARTITION BY type ORDER BY count DESC) type_rank,\n",
    "        count / SUM(count) OVER(PARTITION BY type) percentage\n",
    "    FROM\n",
    "        types_count\n",
    "    WHERE\n",
    "        count > 100\n",
    ")\n",
    "SELECT\n",
    "    type,\n",
    "    ward,\n",
    "    count,\n",
    "    type_rank,\n",
    "    CAST(percentage AS DECIMAL(4,3))\n",
    "FROM types_rank_table\n",
    "WHERE type_rank < 4 AND percentage > 0.1\n",
    "ORDER BY type, type_rank\n",
    "\"\"\"\n",
    "spark.sql(types_top_query).show(truncate=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
