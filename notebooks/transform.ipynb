{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "Testing ground for service call data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import folium\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 10:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- First 3 Chars of Postal Code: string (nullable = true)\n",
      " |-- Intersection Street 1: string (nullable = true)\n",
      " |-- Intersection Street 2: string (nullable = true)\n",
      " |-- Service Request Type: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- ward_name: string (nullable = true)\n",
      " |-- ward_id: byte (nullable = true)\n",
      " |-- creation_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq_path = Path(\"../tests/resources/SR2020.parquet\")\n",
    "df = spark.read.parquet(str(pq_path))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop current SparkContext before creating a new one\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from cloud storage using `gcs-connector-hadoop3` jar\n",
    "\n",
    "Before starting a new context, I had to restart the kernel for spark to recognize the gcs-connector jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# !mkdir ../data/lib\n",
    "# !gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar  \\\n",
    "#     ../data/lib/gcs-connector-hadoop3-latest.jar  \n",
    "# !curl -O https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.11/gcs-connector-hadoop3-2.2.11-shaded.jar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:29:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .setAppName(\"test_cloud\")\n",
    "    .set(\"spark.jars\", \"../data/lib/gcs-connector-hadoop3-latest.jar\")\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\n",
    "    \"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    ")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "spark = SparkSession.builder.config(conf=sc.getConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs_path: gs://service-calls-data-lake/raw/pq/sr2023.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- First 3 Chars of Postal Code: string (nullable = true)\n",
      " |-- Intersection Street 1: string (nullable = true)\n",
      " |-- Intersection Street 2: string (nullable = true)\n",
      " |-- Service Request Type: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- ward_name: string (nullable = true)\n",
      " |-- ward_id: byte (nullable = true)\n",
      " |-- creation_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")\n",
    "gcs_path = f\"gs://{DATA_LAKE}/raw/pq/sr2023.parquet\"\n",
    "print(f\"gcs_path: {gcs_path}\")\n",
    "df_gcs = spark.read.parquet(gcs_path)\n",
    "df_gcs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "How should we model our service call data?\n",
    "\n",
    "Most important fields:\n",
    "\n",
    "- Service Request Type\n",
    "- First 3 Chars of Postal Code\n",
    "- ward_name\n",
    "- datetime\n",
    "\n",
    "Ideas:\n",
    "\n",
    "- most/least common types per month?\n",
    "- most/least common types by ward/FSA?\n",
    "- highest frequency ward/FSA by service types, i.e. which ward has the most/least amount of noise complaints?\n",
    "- combine with population per FSA and find requests per capita?\n",
    "- distributions of the all/top 80% of request types\n",
    "    - filter by ward/FSA/season?\n",
    "\n",
    "Division and section could be queried against to show which municipal entity receive the most requests, although that would highly correlate with the type of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 323880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "load_dotenv()\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")\n",
    "gcs_path = f\"gs://{DATA_LAKE}/raw/pq/SR2021.parquet\"\n",
    "df = spark.read.parquet(gcs_path)\n",
    "print(f\"row count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirement failed: Currently correlation calculation for columns with dataType string not supported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.corr(\"Service Request Type\", \"Division\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common request type by ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------------+-----+\n",
      "|               ward_name|                   Service Request Type|count|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141|\n",
      "|         Don Valley East|                     Property Standards|  349|\n",
      "|        Don Valley North|                                 Zoning|  413|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413|\n",
      "|         Don Valley West|                        General Pruning|  575|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|\n",
      "|         Etobicoke North|                     Property Standards|  517|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|\n",
      "|Humber River-Black Creek|                     Property Standards|  499|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531|\n",
      "|   Scarborough Southwest|                     Property Standards|  701|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|\n",
      "|   Scarborough-Guildwood|                     Property Standards|  484|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|\n",
      "|       Spadina-Fort York|                                 Zoning|  601|\n",
      "|          Toronto Centre|                    INJUR/DIST WILDLIFE|  651|\n",
      "|          Toronto Centre|                     Property Standards|  651|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|\n",
      "|              Willowdale|              Bin Investigation Request|  410|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459|\n",
      "|       York South-Weston|                     Property Standards|  716|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# request type count by ward\n",
    "df_ward_type = (\n",
    "    df.filter(df.ward_name.isNotNull())\n",
    "    .select(\"creation_datetime\", \"ward_name\", \"Service Request Type\")\n",
    "    .withColumnRenamed(\"Service Request Type\", \"service_request_type\")\n",
    "    .groupBy([\"ward_name\", \"service_request_type\"])\n",
    "    .count()\n",
    ")\n",
    "# most common request type's count by ward\n",
    "df_ward_max = (\n",
    "    df_ward_type \\\n",
    "    .groupBy(\"ward_name\")\n",
    "    .max(\"count\")\n",
    "    .withColumnRenamed(\"max(count)\", \"max_count\")\n",
    ")\n",
    "\n",
    "# trying to join them get get most common type by ward\n",
    "df_ward = (\n",
    "    df_ward_max.join(df_ward_type, col(\"max_count\") == col(\"count\") & col(, \"inner\")\n",
    "    .select(\"df_ward_max.ward_name\", \"Service Request Type\", \"count\")\n",
    "    .orderBy(\"ward_name\")\n",
    ")\n",
    "df_ward.show(n=30, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing some repeats of ward name due to counts of different request types being equal; is that a coincidence or is that really true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ward_max.orderBy('ward_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ward.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `Window` instead, as explained in [this SOF post](https://stackoverflow.com/questions/48829993/groupby-column-and-filter-rows-with-maximum-value-in-pyspark)\n",
    "\n",
    "- find max service type by ward\n",
    "- assign that value as a new column, according to ward\n",
    "- filter for rows where count == max_count\n",
    "- that row is the max_count service type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------------+-----+\n",
      "|               ward_name|                   Service Request Type|count|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141|\n",
      "|         Don Valley East|                     Property Standards|  349|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413|\n",
      "|         Don Valley West|                        General Pruning|  575|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|\n",
      "|         Etobicoke North|                     Property Standards|  517|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|\n",
      "|Humber River-Black Creek|                     Property Standards|  499|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531|\n",
      "|   Scarborough Southwest|                     Property Standards|  701|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|\n",
      "|       Spadina-Fort York|                                 Zoning|  601|\n",
      "|          Toronto Centre|                     Property Standards|  651|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459|\n",
      "|       York South-Weston|                     Property Standards|  716|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy('ward_name')\n",
    "df_ward_type.withColumn('max_count', F.max('count').over(w)) \\\n",
    "    .where(F.col('count') == F.col('max_count')) \\\n",
    "    .drop('max_count') \\\n",
    "    .show(n=25, truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "| ward_name|Service Request Type|count|\n",
      "+----------+--------------------+-----+\n",
      "|Willowdale|Long Grass and Weeds|  410|\n",
      "|Willowdale|Residential: Bin:...|  386|\n",
      "|Willowdale|     General Pruning|  375|\n",
      "|Willowdale|    CADAVER WILDLIFE|  313|\n",
      "|Willowdale| INJUR/DIST WILDLIFE|  285|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ward_type \\\n",
    "    .filter(df_ward_type.ward_name == 'Willowdale') \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems my first approach picked up some erroneous rows. Simply matching the max_count and hoping they were unique was perhaps not the best way to go about it.\n",
    "\n",
    "## Window function\n",
    "\n",
    "Characterized by `OVER (PARTITION BY ...)`, there are 3 types:\n",
    "\n",
    "1. Aggregate\n",
    "1. Ranking\n",
    "1. Value\n",
    "\n",
    "### Aggregate\n",
    "\n",
    "Aggregates refer to:\n",
    "\n",
    "- AVG\n",
    "- MAX\n",
    "- MIN\n",
    "- SUM\n",
    "- COUNT\n",
    "\n",
    "This can be a replacement for `GROUP BY`, if we specify that group in the `OVER` clause. But unlike `GROUPBY.COUNT` which collapses the table into those groups, window assigns that value to each row of that dataset.\n",
    "\n",
    "In SQL for above query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_sql_view')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part one - creating table with count of each type for each ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                ward|                type|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|           Davenport|Residential: Bin:...| 1141|\n",
      "|   Eglinton-Lawrence|Residential: Bin:...|  984|\n",
      "|  Parkdale-High Park|Residential: Bin:...|  971|\n",
      "| Etobicoke-Lakeshore|     General Pruning|  913|\n",
      "|  Toronto-St. Paul's|Residential: Bin:...|  895|\n",
      "|    Etobicoke Centre|     General Pruning|  845|\n",
      "|    Toronto-Danforth|     General Pruning|  818|\n",
      "| University-Rosedale|Residential: Bin:...|  810|\n",
      "| Etobicoke-Lakeshore| INJUR/DIST WILDLIFE|  799|\n",
      "|   Beaches-East York|    CADAVER WILDLIFE|  788|\n",
      "|   Beaches-East York| INJUR/DIST WILDLIFE|  784|\n",
      "|   Beaches-East York|Residential: Bin:...|  774|\n",
      "|   Eglinton-Lawrence|     General Pruning|  751|\n",
      "|   Beaches-East York|     General Pruning|  737|\n",
      "|    Toronto-Danforth|    CADAVER WILDLIFE|  728|\n",
      "|   York South-Weston|  Property Standards|  716|\n",
      "|Scarborough South...|  Property Standards|  701|\n",
      "|    Toronto-Danforth|Residential: Bin:...|  699|\n",
      "|Scarborough-Rouge...|     General Pruning|  687|\n",
      "|   Beaches-East York|  Property Standards|  683|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "df_type_count = spark.sql(query)\n",
    "df_type_count.createOrReplaceTempView('df_type_count')\n",
    "df_type_count.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part two - using common table expression (CTE) to add a column with max(count) of each ward, and query against it to find the max count service type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                ward|                type|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|   Beaches-East York|    CADAVER WILDLIFE|  788|\n",
      "|           Davenport|Residential: Bin:...| 1141|\n",
      "|     Don Valley East|  Property Standards|  349|\n",
      "|    Don Valley North|Residential: Bin:...|  413|\n",
      "|     Don Valley West|     General Pruning|  575|\n",
      "|   Eglinton-Lawrence|Residential: Bin:...|  984|\n",
      "|    Etobicoke Centre|     General Pruning|  845|\n",
      "|     Etobicoke North|  Property Standards|  517|\n",
      "| Etobicoke-Lakeshore|     General Pruning|  913|\n",
      "|Humber River-Blac...|  Property Standards|  499|\n",
      "|  Parkdale-High Park|Residential: Bin:...|  971|\n",
      "|  Scarborough Centre|Residential: Bin:...|  668|\n",
      "|   Scarborough North|Residential: Bin:...|  531|\n",
      "|Scarborough South...|  Property Standards|  701|\n",
      "|Scarborough-Aginc...|     General Pruning|  428|\n",
      "|Scarborough-Guild...| INJUR/DIST WILDLIFE|  484|\n",
      "|Scarborough-Rouge...|     General Pruning|  687|\n",
      "|   Spadina-Fort York|              Zoning|  601|\n",
      "|      Toronto Centre|  Property Standards|  651|\n",
      "|    Toronto-Danforth|     General Pruning|  818|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH type_max AS (\n",
    "    SELECT \n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        MAX(count) OVER (PARTITION BY ward) maxcount\n",
    "    FROM df_type_count\n",
    ")\n",
    "SELECT\n",
    "    ward, type, count\n",
    "FROM\n",
    "    type_max\n",
    "WHERE count == maxcount\n",
    "\"\"\"\n",
    "df_type_max = spark.sql(query)\n",
    "df_type_max.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:47:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 14:47:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/22 14:47:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------------------------+---------------------------------------+-----+---+\n",
      "|                    ward|                                   type|count|row|\n",
      "+------------------------+---------------------------------------+-----+---+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|  1|\n",
      "|         Don Valley West|                        General Pruning|  575|  2|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|  3|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|  4|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|  5|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|  6|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|  7|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|  8|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|  9|\n",
      "|         Don Valley East|                     Property Standards|  349| 10|\n",
      "|         Etobicoke North|                     Property Standards|  517| 11|\n",
      "|Humber River-Black Creek|                     Property Standards|  499| 12|\n",
      "|   Scarborough Southwest|                     Property Standards|  701| 13|\n",
      "|          Toronto Centre|                     Property Standards|  651| 14|\n",
      "|       York South-Weston|                     Property Standards|  716| 15|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141| 16|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413| 17|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984| 18|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971| 19|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668| 20|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531| 21|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895| 22|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810| 23|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459| 24|\n",
      "|       Spadina-Fort York|                                 Zoning|  601| 25|\n",
      "+------------------------+---------------------------------------+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_query = \"\"\"\n",
    "with type_count as (\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "),\n",
    "type_max as (\n",
    "    SELECT \n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        MAX(count) OVER (PARTITION BY ward) maxcount\n",
    "    FROM type_count\n",
    ")\n",
    "SELECT\n",
    "    ward, type, count, row_number() over(ORDER BY type) row\n",
    "FROM\n",
    "    type_max\n",
    "WHERE count == maxcount\n",
    "ORDER BY type\n",
    "\"\"\"\n",
    "df_sql = spark.sql(window_query).show(n=25, truncate=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common ward by request type\n",
    "\n",
    "Inverse of the previous question - which neighborhood makes the most calls for each request type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                type|                ward|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|All / Hazardous W...|  Parkdale-High Park|  656|\n",
      "|Bin Investigation...|           Davenport|  511|\n",
      "|Boulevard - Ploug...|Scarborough-Guild...|  102|\n",
      "|Boulevards - Dama...|    Etobicoke Centre|  194|\n",
      "|By-Law Contravent...| Etobicoke-Lakeshore|  126|\n",
      "|    CADAVER WILDLIFE|   Beaches-East York|  788|\n",
      "|Catch Basin - Blo...|   Eglinton-Lawrence|  134|\n",
      "|Comment / Suggestion|   Spadina-Fort York|  118|\n",
      "|Complaint/Investi...|           Davenport|  179|\n",
      "|Dispute SR Status...|   Eglinton-Lawrence|  211|\n",
      "|      Dogs off Leash|   Spadina-Fort York|  131|\n",
      "|Expressway requir...|   Spadina-Fort York|  122|\n",
      "|     General Pruning| Etobicoke-Lakeshore|  913|\n",
      "|General Tree Main...| Etobicoke-Lakeshore|  115|\n",
      "|            Graffiti| University-Rosedale|  290|\n",
      "|Gypsy Moth Contro...|    Don Valley North|  168|\n",
      "| INJUR/DIST WILDLIFE| Etobicoke-Lakeshore|  799|\n",
      "|Ice and Snow Comp...| University-Rosedale|  143|\n",
      "|Litter / Bin / Ov...|   Spadina-Fort York|  331|\n",
      "|Litter / Illegal ...| University-Rosedale|  323|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ward_query = \"\"\"\n",
    "with type_count as (\n",
    "    SELECT\n",
    "        ward_name ward,\n",
    "        `Service Request Type` type,\n",
    "        COUNT(1) count\n",
    "    FROM\n",
    "        df_sql_view\n",
    "    GROUP BY ward_name, `Service Request Type`\n",
    "    HAVING ward_name IS NOT NULL\n",
    "),\n",
    "ward_max as (\n",
    "    SELECT \n",
    "        ward,\n",
    "        type,\n",
    "        count,\n",
    "        MAX(count) OVER (PARTITION BY type) maxcount\n",
    "    FROM type_count\n",
    ")\n",
    "SELECT\n",
    "    type, ward, count\n",
    "FROM ward_max\n",
    "WHERE maxcount = count and count > 100\n",
    "\"\"\"\n",
    "df_ward_by_type = spark.sql(ward_query).show()\n",
    "# df_ward_by_type\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 requests by season\n",
    "\n",
    "Let's define seasons by datetime:\n",
    "\n",
    "- spring: 3/21-6/21\n",
    "- summer: 6/21-9/21\n",
    "- fall: 9/21 - 11/30 (winter is long, okay)\n",
    "- winter: 12/1 - 3/21\n",
    "\n",
    "Execution:\n",
    "\n",
    "- add season column via UDF\n",
    "- for each season, count rows by service type\n",
    "- since we want the top $n$ service type by season, we can't just use `max`\n",
    "    - for each season, sort the count\n",
    "    - `row_number() OVER(PARTITION BY season)`\n",
    "    - get row numbers 1-5 for each season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  creation_datetime|\n",
      "+-------------------+\n",
      "|2021-01-01 20:37:54|\n",
      "|2021-01-02 15:34:08|\n",
      "|2021-01-02 15:42:29|\n",
      "|2021-01-03 09:25:41|\n",
      "|2021-01-03 14:00:47|\n",
      "|2021-01-03 14:52:53|\n",
      "|2021-01-03 16:42:20|\n",
      "|2021-01-03 16:59:58|\n",
      "|2021-01-04 10:08:03|\n",
      "|2021-01-04 11:08:34|\n",
      "|2021-01-04 11:29:09|\n",
      "|2021-01-04 13:47:57|\n",
      "|2021-01-04 18:38:14|\n",
      "|2021-01-04 18:49:55|\n",
      "|2021-01-05 09:35:50|\n",
      "|2021-01-05 10:15:37|\n",
      "|2021-01-05 10:17:07|\n",
      "|2021-01-05 12:47:21|\n",
      "|2021-01-05 14:24:13|\n",
      "|2021-01-05 16:50:05|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dt_samp, _ = df.randomSplit([0.1,0.9])\n",
    "dt_samp = df.sample(0.005).select('creation_datetime')\n",
    "dt_samp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1699"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_samp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1 = dt_samp.head(1)\n",
    "type(dt1[0].creation_datetime)\n",
    "dtval = dt1[0].creation_datetime\n",
    "dtval.year\n",
    "spring = datetime(dtval.year, month=3, day=21)\n",
    "dtval < spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_season(creation_time: datetime) -> str:\n",
    "    \"\"\"Extracts season from the datetime field\"\"\"\n",
    "    m = creation_time.month\n",
    "    d = creation_time.day\n",
    "    if m < 3 or (m == 3 and d < 21) or m > 11:\n",
    "        return 'winter'\n",
    "    elif m < 6 or (m == 6 and d < 21):\n",
    "        return 'spring'\n",
    "    elif m < 9 or (m == 9 and d < 21):\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'fall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_season_udf = F.udf(extract_season, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|  creation_datetime|season|\n",
      "+-------------------+------+\n",
      "|2021-02-01 14:33:56|winter|\n",
      "|2021-02-25 18:34:44|winter|\n",
      "|2021-03-01 15:46:41|winter|\n",
      "|2021-03-11 09:23:17|winter|\n",
      "|2021-04-12 12:58:47|spring|\n",
      "|2021-04-21 12:33:16|spring|\n",
      "|2021-04-28 16:21:34|spring|\n",
      "|2021-05-06 13:34:14|spring|\n",
      "|2021-05-11 09:03:35|spring|\n",
      "|2021-05-13 15:09:33|spring|\n",
      "|2021-05-31 09:32:44|spring|\n",
      "|2021-06-03 09:24:27|spring|\n",
      "|2021-07-09 10:38:02|summer|\n",
      "|2021-07-15 18:13:51|summer|\n",
      "|2021-08-06 17:47:59|summer|\n",
      "|2021-08-09 12:12:22|summer|\n",
      "|2021-08-11 18:22:40|summer|\n",
      "|2021-08-16 11:00:34|summer|\n",
      "|2021-08-16 13:15:40|summer|\n",
      "|2021-08-27 12:03:40|summer|\n",
      "|2021-09-08 14:12:10|summer|\n",
      "|2021-09-13 11:03:23|summer|\n",
      "|2021-09-16 15:05:24|summer|\n",
      "|2021-09-20 08:09:53|summer|\n",
      "|2021-10-11 22:39:56|  fall|\n",
      "|2021-10-12 08:26:15|  fall|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_samp.withColumn('season', extract_season_udf(dt_samp.creation_datetime)) \\\n",
    "    .sample(0.02).show(n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
