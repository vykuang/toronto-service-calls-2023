{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "Testing ground for service call data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 10:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- First 3 Chars of Postal Code: string (nullable = true)\n",
      " |-- Intersection Street 1: string (nullable = true)\n",
      " |-- Intersection Street 2: string (nullable = true)\n",
      " |-- Service Request Type: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- ward_name: string (nullable = true)\n",
      " |-- ward_id: byte (nullable = true)\n",
      " |-- creation_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pq_path = Path(\"../tests/resources/SR2020.parquet\")\n",
    "df = spark.read.parquet(str(pq_path))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop current SparkContext before creating a new one\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from cloud storage using `gcs-connector-hadoop3` jar\n",
    "\n",
    "Before starting a new context, I had to restart the kernel for spark to recognize the gcs-connector jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../data/lib\n",
    "!gsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar  \\\n",
    "    ../data/lib/gcs-connector-hadoop3-latest.jar  \n",
    "# !curl -O https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/download/v2.2.11/gcs-connector-hadoop3-2.2.11-shaded.jar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 13:41:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    SparkConf()\n",
    "    .setMaster(\"local[*]\")\n",
    "    .setAppName(\"test_cloud\")\n",
    "    .set(\"spark.jars\", \"../data/lib/gcs-connector-hadoop3-latest.jar\")\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\n",
    "    \"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\"\n",
    ")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "spark = SparkSession.builder.config(conf=sc.getConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs_path: gs://service-calls-data-lake/raw/pq/sr2023.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Status: string (nullable = true)\n",
      " |-- First 3 Chars of Postal Code: string (nullable = true)\n",
      " |-- Intersection Street 1: string (nullable = true)\n",
      " |-- Intersection Street 2: string (nullable = true)\n",
      " |-- Service Request Type: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- ward_name: string (nullable = true)\n",
      " |-- ward_id: byte (nullable = true)\n",
      " |-- creation_datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "DATA_LAKE = os.getenv(\"DATA_LAKE\")\n",
    "gcs_path = f\"gs://{DATA_LAKE}/raw/pq/sr2023.parquet\"\n",
    "print(f\"gcs_path: {gcs_path}\")\n",
    "df_gcs = spark.read.parquet(gcs_path)\n",
    "df_gcs.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "How should we model our service call data?\n",
    "\n",
    "Most important fields:\n",
    "\n",
    "- Service Request Type\n",
    "- First 3 Chars of Postal Code\n",
    "- ward_name\n",
    "- datetime\n",
    "\n",
    "Ideas:\n",
    "\n",
    "- most/least common types per month?\n",
    "- most/least common types by ward/FSA?\n",
    "- highest frequency ward/FSA by service types, i.e. which ward has the most/least amount of noise complaints?\n",
    "- combine with population per FSA and find requests per capita?\n",
    "- distributions of the all/top 80% of request types\n",
    "    - filter by ward/FSA/season?\n",
    "\n",
    "Division and section could be queried against to show which municipal entity receive the most requests, although that would highly correlate with the type of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 323880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "gcs_path = f\"gs://{DATA_LAKE}/raw/pq/SR2021.parquet\"\n",
    "df = spark.read.parquet(gcs_path)\n",
    "print(f\"row count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirement failed: Currently correlation calculation for columns with dataType string not supported.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.corr(\"Service Request Type\", \"Division\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------------+-----+\n",
      "|               ward_name|                   Service Request Type|count|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "|       Beaches-East York|                       CADAVER WILDLIFE|  788|\n",
      "|               Davenport|Residential: Bin: Repair or Replace Lid| 1141|\n",
      "|         Don Valley East|                     Property Standards|  349|\n",
      "|        Don Valley North|                                 Zoning|  413|\n",
      "|        Don Valley North|Residential: Bin: Repair or Replace Lid|  413|\n",
      "|         Don Valley West|                        General Pruning|  575|\n",
      "|       Eglinton-Lawrence|Residential: Bin: Repair or Replace Lid|  984|\n",
      "|        Etobicoke Centre|                        General Pruning|  845|\n",
      "|         Etobicoke North|                     Property Standards|  517|\n",
      "|     Etobicoke-Lakeshore|                        General Pruning|  913|\n",
      "|Humber River-Black Creek|                     Property Standards|  499|\n",
      "|      Parkdale-High Park|Residential: Bin: Repair or Replace Lid|  971|\n",
      "|      Scarborough Centre|Residential: Bin: Repair or Replace Lid|  668|\n",
      "|       Scarborough North|Residential: Bin: Repair or Replace Lid|  531|\n",
      "|   Scarborough Southwest|                     Property Standards|  701|\n",
      "|   Scarborough-Agincourt|                        General Pruning|  428|\n",
      "|   Scarborough-Guildwood|                     Property Standards|  484|\n",
      "|   Scarborough-Guildwood|                    INJUR/DIST WILDLIFE|  484|\n",
      "|  Scarborough-Rouge Park|                        General Pruning|  687|\n",
      "|       Spadina-Fort York|                                 Zoning|  601|\n",
      "|          Toronto Centre|                    INJUR/DIST WILDLIFE|  651|\n",
      "|          Toronto Centre|                     Property Standards|  651|\n",
      "|        Toronto-Danforth|                        General Pruning|  818|\n",
      "|      Toronto-St. Paul's|Residential: Bin: Repair or Replace Lid|  895|\n",
      "|     University-Rosedale|Residential: Bin: Repair or Replace Lid|  810|\n",
      "|              Willowdale|                   Long Grass and Weeds|  410|\n",
      "|              Willowdale|              Bin Investigation Request|  410|\n",
      "|             York Centre|Residential: Bin: Repair or Replace Lid|  459|\n",
      "|       York South-Weston|                     Property Standards|  716|\n",
      "+------------------------+---------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# request type count by ward\n",
    "df_ward_type = (\n",
    "    df.filter(df.ward_name.isNotNull())\n",
    "    .select(\"creation_datetime\", \"ward_name\", \"Service Request Type\")\n",
    "    .groupBy([\"ward_name\", \"Service Request Type\"])\n",
    "    .count()\n",
    ")\n",
    "# most common request type's count by ward\n",
    "df_ward_max = (\n",
    "    df_ward_type.alias(\"df_ward_max\")\n",
    "    .groupBy(\"ward_name\")\n",
    "    .max(\"count\")\n",
    "    .withColumnRenamed(\"max(count)\", \"max_count\")\n",
    ")\n",
    "\n",
    "# trying to join them get get most common type by ward\n",
    "df_ward = (\n",
    "    df_ward_max.join(df_ward_type, col(\"max_count\") == col(\"count\"), \"inner\")\n",
    "    .select(\"df_ward_max.ward_name\", \"Service Request Type\", \"count\")\n",
    "    .orderBy(\"ward_name\")\n",
    ")\n",
    "df_ward.show(n=30, truncate=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing some repeats of ward name due to counts of different request types being equal; is that a coincidence or is that really true?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "to-service-qAcdV8BS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
